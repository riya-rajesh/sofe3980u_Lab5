{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CleanLab\n"
      ],
      "metadata": {
        "id": "JDtfBqbeioJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installing CleanLab with DataLab Extension"
      ],
      "metadata": {
        "id": "VwDQSPgditTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command installs the cleanlab package, which is used for identifying and correcting label issues in datasets. The [datalab] extra installs additional dependencies required for data exploration and visualization."
      ],
      "metadata": {
        "id": "cBqFkizQjG2G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FslSnc5HyvwV"
      },
      "outputs": [],
      "source": [
        "!pip install \"cleanlab[datalab]\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Importing Required Libraries"
      ],
      "metadata": {
        "id": "abLaruQwiwsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, several essential libraries are imported:\n",
        "\n",
        "    - numpy and pandas for numerical computations and data handling.\n",
        "    - load_iris to load the Iris dataset.\n",
        "    - train_test_split to split the dataset into training and testing sets.\n",
        "    - RandomForestClassifier to define a classifier model.\n",
        "    - CleanLearning from cleanlab.classification is imported to perform anomaly detection and label correction."
      ],
      "metadata": {
        "id": "nc-UVbtRjOM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from cleanlab.classification import CleanLearning"
      ],
      "metadata": {
        "id": "_k1Zbeus_l78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Loading Iris Dataset and Introducing Label Errors\n"
      ],
      "metadata": {
        "id": "xbAe5p8AizfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step loads the Iris dataset, which consists of 150 samples of iris flowers classified into three species. We then introduce errors in the labels (y) by randomly selecting 5 indices and replacing their true labels with incorrect ones. This simulates mislabeled data points, which are then detected later in the process."
      ],
      "metadata": {
        "id": "E_yemcCOjT0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "np.random.seed(42)\n",
        "num_errors = 5\n",
        "error_indices = np.random.choice(len(y), num_errors, replace=False)\n",
        "y[error_indices] = np.random.choice([0, 1, 2], num_errors, replace=True)"
      ],
      "metadata": {
        "id": "sOkaSTMh_pyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Splitting the Dataset into Training and Test Sets"
      ],
      "metadata": {
        "id": "vx9vlDCMi2_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is split into training (X_train, y_train) and testing (X_test, y_test) sets. 80% of the data is used for training, and 20% is reserved for testing. The random_state=42 ensures that the split is reproducible."
      ],
      "metadata": {
        "id": "WllS92ifjWsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "UTl_o3ft_q8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Defining and Fitting the Random Forest Classifier with Cleanlab's CleanLearning"
      ],
      "metadata": {
        "id": "eCtXYchYi5ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Random Forest classifier is defined with 100 trees. The classifier is then wrapped in CleanLearning from cleanlab, which allows the model to automatically detect and correct label issues in the dataset. The model is trained using the fit() method on the training data (X_train, y_train).\n"
      ],
      "metadata": {
        "id": "Eb3j8Ouijain"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Use Cleanlab's CleanLearning\n",
        "cleaner = CleanLearning(clf)\n",
        "cleaner.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "qYrzr0En_rIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Identifying Suspected Label Issues"
      ],
      "metadata": {
        "id": "lsX-_u4Ci713"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The find_label_issues() method identifies potential mislabeled data points based on the classifier’s predictions. It returns a label_issues object with a boolean flag (is_label_issue) for each data point. The indices of the suspected mislabeled points are extracted and printed for review."
      ],
      "metadata": {
        "id": "e-CRd5Mxjogq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get label issues (higher scores mean more likely mislabeled)\n",
        "label_issues = cleaner.find_label_issues(X=X_train, labels=y_train)\n",
        "\n",
        "# Display suspected mislabeled data\n",
        "mislabeled_indices = np.where(label_issues[\"is_label_issue\"])[0]\n",
        "print(\"Suspected label errors at indices:\", mislabeled_indices)"
      ],
      "metadata": {
        "id": "F_5_-Y4a_t2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Displaying Suspected Mislabeled Data Points"
      ],
      "metadata": {
        "id": "0Q5uRUMzi96p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After detecting the mislabeled points, the predict() method is used to obtain the model’s predictions on the training data. For each suspected mislabeled index, a DataFrame is created with the following columns:\n",
        "\n",
        "    - The feature values (X_train[idx]).\n",
        "    - The true label (y_train[idx]).\n",
        "    - The label previously assigned by the model (predicted_labels[idx])."
      ],
      "metadata": {
        "id": "-_OgdmkLjqUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each individual DataFrame containing a suspected mislabeled data point is appended to the suspect_dfs list. Once all suspected points are processed, the list of DataFrames is concatenated into a single DataFrame (df_all_suspects). This final DataFrame is then printed to display all the suspected mislabeled data points in a clean, readable table format."
      ],
      "metadata": {
        "id": "89QtUOHZj7xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model's predictions\n",
        "predicted_labels = cleaner.predict(X_train)\n",
        "\n",
        "# Create an empty list to store DataFrames\n",
        "suspect_dfs = []\n",
        "\n",
        "# Loop over the mislabeled indices and create a structured DataFrame for each\n",
        "for idx in mislabeled_indices:\n",
        "    # Create a DataFrame for the suspected mislabeled data point\n",
        "    df_suspect = pd.DataFrame([X_train[idx]], columns=iris.feature_names)\n",
        "    df_suspect.insert(0, \"Index\", idx)  # Insert index column\n",
        "    df_suspect[\"True Label\"] = y_train[idx]  # Correct label\n",
        "    df_suspect[\"Previously Assigned Label\"] = predicted_labels[idx]  # What it was classified as before\n",
        "\n",
        "    # Append the current suspect DataFrame to the list\n",
        "    suspect_dfs.append(df_suspect)\n",
        "\n",
        "# Combine all the suspect DataFrames into a single DataFrame\n",
        "df_all_suspects = pd.concat(suspect_dfs, ignore_index=True)\n",
        "\n",
        "# Print the full table of suspected mislabeled data points\n",
        "print(\"\\n                                           Suspected Mislabeled Data Points\")\n",
        "print(\"-----------------------------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(df_all_suspects.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "mSWUrsSP6Aqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}